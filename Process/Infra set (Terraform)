# Create .ssh directory if it doesn't exist
mkdir -p ~/.ssh

# Generate key for App Server
ssh-keygen -t ed25519 -f ~/.ssh/appserver-key -C "app-server-key" -N ""

# Generate key for DB Server
ssh-keygen -t ed25519 -f ~/.ssh/dbserver-key -C "db-server-key" -N ""

# Set correct permissions
chmod 600 ~/.ssh/appserver-key
chmod 600 ~/.ssh/dbserver-key
chmod 644 ~/.ssh/appserver-key.pub
chmod 644 ~/.ssh/dbserver-key.pub


echo "appserver-key" >> .gitignore
echo "dbserver-key" >> .gitignore
echo "*.pub" >> .gitignore


mkdir tailadmin-terraform
cd tailadmin-terraform

mkdir -p terraform/{modules,environments/dev}

vim provider.tf

terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region     = var.aws_region
  access_key = var.aws_access_key
  secret_key = var.aws_secret_key
}


vim variables.tf

variable "aws_region" {
  description = "AWS Region"
  type        = string
  default     = "us-east-1"
}

variable "aws_access_key" {
  description = "AWS Access Key"
  type        = string
  sensitive   = true
}

variable "aws_secret_key" {
  description = "AWS Secret Key"
  type        = string
  sensitive   = true
}

variable "project_name" {
  description = "Project name"
  type        = string
  default     = "TailAdmin"
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "dev"
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t3.micro"
}

variable "db_instance_type" {
  description = "Database EC2 instance type"
  type        = string
  default     = "t3.micro"
}

# Just the key names, not the actual keys
variable "app_key_name" {
  description = "SSH key pair name for app server"
  type        = string
  default     = "appserver-key"
}

variable "db_key_name" {
  description = "SSH key pair name for DB server"
  type        = string
  default     = "dbserver-key"
}





vim terraform.tfvars

aws_region       = "us-east-1"
aws_access_key   = "AKIAT22GNOQUFVIMPVR5"
aws_secret_key   = "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"
project_name     = "TailAdmin"
environment      = "dev"
instance_type    = "t3.micro"
db_instance_type = "t3.micro"
app_key_name     = "appserver-key"
db_key_name      = "dbserver-key"



echo "terraform.tfvars" >> .gitignore
echo "*.tfstate*" >> .gitignore
echo ".terraform*" >> .gitignore


vim data.tf

# Get default VPC
data "aws_vpc" "default" {
  default = true
}

# Get default subnets
data "aws_subnets" "default" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.default.id]
  }
}

# Get first subnet for app server
data "aws_subnet" "app_subnet" {
  id = data.aws_subnets.default.ids[0]
}

# Get second subnet for db server (if available)
data "aws_subnet" "db_subnet" {
  id = length(data.aws_subnets.default.ids) > 1 ? data.aws_subnets.default.ids[1] : data.aws_subnets.default.ids[0]
}

# Get latest Ubuntu AMI
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}



vim locals.tf

locals {
  # Read SSH public keys from files
  app_ssh_public_key = file("${path.module}/appserver-key.pub")
  db_ssh_public_key  = file("${path.module}/dbserver-key.pub")
  
  # Or read from home directory (use absolute path)
  # app_ssh_public_key = file(pathexpand("~/.ssh/appserver-key.pub"))
  # db_ssh_public_key  = file(pathexpand("~/.ssh/dbserver-key.pub"))
  
  # DT user provided key
  dt_ssh_public_key = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICV/wCuwCL2hgXodxQBFcyJd/rurJfo+Gl90QVu5AL2M dt-home-task-key"
}




vim security_groups.tf

# Security Group for Application Server
resource "aws_security_group" "app_sg" {
  name        = "${var.project_name}-App-SG"
  description = "Security group for application server"
  vpc_id      = data.aws_vpc.default.id

  # HTTP
  ingress {
    description = "HTTP from anywhere"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # HTTPS
  ingress {
    description = "HTTPS from anywhere"
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # SSH
  ingress {
    description = "SSH from anywhere"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # All outbound traffic
  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "${var.project_name}-App-SG"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# Security Group for Database Server
resource "aws_security_group" "db_sg" {
  name        = "${var.project_name}-DB-SG"
  description = "Security group for database server"
  vpc_id      = data.aws_vpc.default.id

  # MySQL from App SG only
  ingress {
    description     = "MySQL from application server"
    from_port       = 3306
    to_port         = 3306
    protocol        = "tcp"
    security_groups = [aws_security_group.app_sg.id]
  }

  # SSH from App SG only
  ingress {
    description     = "SSH from application server"
    from_port       = 22
    to_port         = 22
    protocol        = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # All outbound traffic
  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "${var.project_name}-DB-SG"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}





vim key_pair.tf

# Create AWS Key Pair for App Server
resource "aws_key_pair" "app_key" {
  key_name   = var.app_key_name
  public_key = local.app_ssh_public_key

  tags = {
    Name        = "${var.project_name}-App-KeyPair"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# Create AWS Key Pair for DB Server
resource "aws_key_pair" "db_key" {
  key_name   = var.db_key_name
  public_key = local.db_ssh_public_key

  tags = {
    Name        = "${var.project_name}-DB-KeyPair"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}


vim iam.tf

# IAM Role for EC2 instances (for CodeDeploy, S3 access, etc.)
resource "aws_iam_role" "ec2_role" {
  name = "${var.project_name}-EC2-Role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })

  tags = {
    Name        = "${var.project_name}-EC2-Role"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# Attach policies to EC2 role
resource "aws_iam_role_policy_attachment" "ec2_codedeploy" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforAWSCodeDeploy"
}

resource "aws_iam_role_policy_attachment" "ec2_s3_readonly" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
}

resource "aws_iam_role_policy_attachment" "ec2_ecr_readonly" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

resource "aws_iam_role_policy_attachment" "ec2_ssm" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

# Instance Profile
resource "aws_iam_instance_profile" "ec2_profile" {
  name = "${var.project_name}-EC2-Profile"
  role = aws_iam_role.ec2_role.name

  tags = {
    Name        = "${var.project_name}-EC2-Profile"
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}



vim ec2.tf

# Application Server
resource "aws_instance" "app_server" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = var.instance_type
  key_name               = aws_key_pair.app_key.key_name
  subnet_id              = data.aws_subnet.app_subnet.id
  vpc_security_group_ids = [aws_security_group.app_sg.id]
  iam_instance_profile   = aws_iam_instance_profile.ec2_profile.name

  root_block_device {
    volume_size           = 20
    volume_type           = "gp3"
    delete_on_termination = true
    encrypted             = false
  }

  user_data = templatefile("${path.module}/user-data-app.sh", {
    db_private_ip     = aws_instance.db_server.private_ip
    dt_ssh_public_key = local.dt_ssh_public_key
  })

  tags = {
    Name        = "Application-Server"
    Environment = var.environment
    ManagedBy   = "Terraform"
    Role        = "Application"
  }

  depends_on = [aws_instance.db_server]
}

# Database Server
resource "aws_instance" "db_server" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = var.db_instance_type
  key_name               = aws_key_pair.db_key.key_name
  subnet_id              = data.aws_subnet.db_subnet.id
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  iam_instance_profile   = aws_iam_instance_profile.ec2_profile.name

  root_block_device {
    volume_size           = 20
    volume_type           = "gp3"
    delete_on_termination = true
    encrypted             = false
  }

  user_data = file("${path.module}/user-data-db.sh")

  tags = {
    Name        = "DB-Server"
    Environment = var.environment
    ManagedBy   = "Terraform"
    Role        = "Database"
  }
}




vim user-data-app.sh

#!/bin/bash
set -e

# Log everything
exec > >(tee /var/log/user-data.log)
exec 2>&1

echo "Starting application server setup..."

# Update system
apt-get update
DEBIAN_FRONTEND=noninteractive apt-get upgrade -y

# Install Docker
apt-get install -y docker.io
systemctl start docker
systemctl enable docker
usermod -aG docker ubuntu

# Install Docker Compose
curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

# Install Git
apt-get install -y git

# Install AWS CLI
apt-get install -y awscli

# Install CodeDeploy Agent
apt-get install -y ruby-full wget
cd /home/ubuntu
wget https://aws-codedeploy-us-east-1.s3.us-east-1.amazonaws.com/latest/install
chmod +x ./install
./install auto
service codedeploy-agent start
systemctl enable codedeploy-agent

# Create application directory
mkdir -p /home/ubuntu/tailadmin-laravel
chown -R ubuntu:ubuntu /home/ubuntu/tailadmin-laravel

# Add dtuser with the provided SSH key
echo "Creating dtuser..."
useradd -m -s /bin/bash dtuser

# Set up SSH for dtuser
mkdir -p /home/dtuser/.ssh
chmod 700 /home/dtuser/.ssh

# Add the provided public key
echo "${dt_ssh_public_key}" > /home/dtuser/.ssh/authorized_keys
chmod 600 /home/dtuser/.ssh/authorized_keys
chown -R dtuser:dtuser /home/dtuser/.ssh

# Add dtuser to docker and sudo groups
usermod -aG docker dtuser
usermod -aG sudo dtuser

# Allow dtuser to use sudo without password (optional, for convenience)
echo "dtuser ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers.d/dtuser
chmod 440 /etc/sudoers.d/dtuser

# Save DB private IP to a file for later use
echo "${db_private_ip}" > /home/ubuntu/db_private_ip.txt
chown ubuntu:ubuntu /home/ubuntu/db_private_ip.txt

echo "Application server setup complete!" 
date >> /var/log/user-data-complete.log




vim user-data-db.sh

#!/bin/bash
set -e

# Update system
apt-get update
apt-get upgrade -y

# Install MySQL
apt-get install -y mysql-server

# Start and enable MySQL
systemctl start mysql
systemctl enable mysql

# Configure MySQL to listen on all interfaces
sed -i 's/bind-address.*/bind-address = 0.0.0.0/' /etc/mysql/mysql.conf.d/mysqld.cnf
systemctl restart mysql

# Set root password and create database
mysql -e "ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'SecureRootPass123!';"
mysql -u root -pSecureRootPass123! -e "CREATE DATABASE IF NOT EXISTS tailadmin_db;"
mysql -u root -pSecureRootPass123! -e "CREATE USER IF NOT EXISTS 'tailadmin_user'@'%' IDENTIFIED BY 'TailAdminPass123!';"
mysql -u root -pSecureRootPass123! -e "GRANT ALL PRIVILEGES ON tailadmin_db.* TO 'tailadmin_user'@'%';"
mysql -u root -pSecureRootPass123! -e "FLUSH PRIVILEGES;"

# Install AWS CLI for backups
apt-get install -y awscli

# Create backup directory
mkdir -p /backups/mysql
chown ubuntu:ubuntu /backups/mysql

echo "Database server setup complete!" > /var/log/user-data.log




vim outputs.tf

output "app_server_public_ip" {
  description = "Application Server Public IP"
  value       = aws_instance.app_server.public_ip
}

output "app_server_private_ip" {
  description = "Application Server Private IP"
  value       = aws_instance.app_server.private_ip
}

output "db_server_private_ip" {
  description = "Database Server Private IP"
  value       = aws_instance.db_server.private_ip
}

output "app_server_id" {
  description = "Application Server Instance ID"
  value       = aws_instance.app_server.id
}

output "db_server_id" {
  description = "Database Server Instance ID"
  value       = aws_instance.db_server.id
}

output "vpc_id" {
  description = "VPC ID"
  value       = data.aws_vpc.default.id
}

output "app_security_group_id" {
  description = "Application Security Group ID"
  value       = aws_security_group.app_sg.id
}

output "db_security_group_id" {
  description = "Database Security Group ID"
  value       = aws_security_group.db_sg.id
}

# SSH commands for different users
output "ssh_command_app_ubuntu" {
  description = "SSH command for Application Server (ubuntu user)"
  value       = "ssh -i ~/.ssh/appserver-key ubuntu@${aws_instance.app_server.public_ip}"
}

output "ssh_command_app_dtuser" {
  description = "SSH command for Application Server (dtuser)"
  value       = "ssh dtuser@${aws_instance.app_server.public_ip}"
}

output "ssh_command_db" {
  description = "SSH command for Database Server"
  value       = "ssh -i ~/.ssh/dbserver-key ubuntu@${aws_instance.db_server.private_ip}"
}

output "app_url" {
  description = "Application URL"
  value       = "http://${aws_instance.app_server.public_ip}"
}

output "key_locations" {
  description = "SSH key file locations"
  value = {
    app_server = "~/.ssh/appserver-key"
    db_server  = "~/.ssh/dbserver-key"
  }
}





terraform init
terraform fmt
terraform validate
terraform plan
terraform apply

# Remove the failed resources from state
terraform state list

# If you see any resources, destroy them first
terraform destroy -auto-approve

